---
title: "home location from land use"
author: "Michael Sinclair"
date: "12/05/2023"
output: html_document
---

load libraries
```{r}
library(tidyjson)
library(ndjson)
library(spatialEco)
library(RSQLite)
library(DBI)
library(dplyr)
library(sp)
library(rgdal)
library(ggplot2)
library(devtools)
library(lubridate)
library(rgdal)
library(gganimate)
library(rnaturalearth)
library(ggmap)
library(sf)
library(rgdal)
library(GISTools)
library(mapview)
library(raster)
library(rdist)
library(raster)
library(sqldf)
library(OpenStreetMap)
library(osmdata)
library(tidyverse)
library(gmapsdistance)
library(ggmap)
library(spatialEco)
library(janitor)
library(GISTools)
library(raster)
library(tidyr)
library(beepr)


```

Method: Using activity heuristics and land use to estimate home location. Estimating home location using enriched data (active evenings between 8pm - 6am for MIXED RESIDENTIAL)

The pre-processing steps in the code 'Complete SQL code.sql' should be run before this script.

load database where pre-joined tables are located
```{r}
db <- "****"
host_db <- "****" 
db_port <- "****"  
db_user <- "****" 
db_password <- "****"
michael_DB <- dbConnect(RPostgres::Postgres(), dbname = db, host=host_db, port=db_port, user=db_user, password=db_password)

```


HUQ

2019

home estimates for all huq using pre-joined evening residential impressions 
```{r}
#get vector of users 
users<- dbGetQuery(michael_DB, "SELECT DISTINCT device_iid_hash FROM public.huq_2019_residential_datazones_evening")
users <- users[,1]

#set up dataframe to populate home location estimates on the sample of users
Huq_homelocations_activeevenings <- data.frame(matrix(ncol = 14, nrow = length(users)))

#name DF fields
names(Huq_homelocations_activeevenings) <- c("Device_iid_hash", "EveResImpressions", "Datazone", "#EveInHome", "ActiveRegions", "#MeanActiveEvenings", "SD_ActiveEvenings", "Latitude", "Longitude", "%EveResInHome", "MeanAccInHome", "MedianHourInHome", "daysindatabase", "TimeActive")
  
#loop through each user ID in sample, subsetting their library of impressions, enriching impressions and returning datazone with most active days. Also returning various metrics useful to determine the accuracy of the result

for (i in 1:length(users)){
    
  message(paste("Estimating home location for user: ", users[i]))
    
    # add user ID to DF
    Huq_homelocations_activeevenings[i,1] <- users[i]
    
    # subset user i's impressions from database via SQL query
    library <-dbGetQuery(michael_DB, 
                         paste("select * from public.huq_2019_residential_datazones_evening where device_iid_hash in ('", users[i],"')", sep = ""))
    
    #check the time in the database
    Huq_homelocations_activeevenings[i,13] <- as.numeric(max(library$timestamp)-min(library$timestamp))
    Huq_homelocations_activeevenings[i,14] <- Huq_homelocations_activeevenings[i,13]/365
    
    #subset ony accuracy under 70m which is around the median + 1 standard deviation from the mean
    library <- subset(library, library$impression_acc <100)
    
    # Convert date field to number in a new seperate colomn and adds time field 
    library$date <- strptime(library$timestamp, format="%Y-%m-%d")
    library$date_num <- as.numeric(library$date)
    library$time <- hour(library$timestamp)
    op <- options(digits.secs = 3)
    options(op)
  
    #count the total impressions from a user and add to DF
    Huq_homelocations_activeevenings[i,2] <- nrow(library)
    
        #if the user has no evening data then add to DF and move onto the next user
    if(nrow(library)<1){
      Huq_homelocations_activeevenings[i,3] <- "Poor accuracy"
      Huq_homelocations_activeevenings[i,4]  <- 0}
    
    else {

    # sorts library by ID and date
    library_ordered <- library[order(library$device_iid_hash, library$date_num, library$datazone), ] 
    
    #Adds columns of activeevenings and inpute 0 to rows to remove and 1 to activeevnings
    library_ordered$activeevenings <- ifelse(library_ordered$device_iid_hash == lag(library_ordered$device_iid_hash) & library_ordered$date_num == lag(library_ordered$date_num) &     library_ordered$datazone == lag(library_ordered$datazone), 0, 1)
    
    # creates activeevenings_index which is TRUE for activeevenings (rows) to keep. This is used as index to create a new DF with specific rows removed
    activeevenings_index <- library_ordered$activeevenings==1 | is.na(library_ordered$activeevenings)
    activeevenings <- library_ordered[activeevenings_index==TRUE, ]
    
    #count which datazone has the most active evenings and index
    result <- activeevenings %>% count(datazone)
    index <- which(result$n == max(result$n))

    # IF a single  datazone is returned: assign datazone code and count of active evenings to DF of users
    if (length(index)==1){
      #add datazone code to DF
      Huq_homelocations_activeevenings[i,3] <- result[index,1]
      #add count of active evenings in datazone
      Huq_homelocations_activeevenings[i,4] <- result[index,2]
      
      ##summary of results for all datazones which a users engaged in (Number of active datazones in a library, mean active data in all zones, SD...etc)
      ##mean activeevneings in datazones
      #subset datazones which the user is active in

      #calculate the mean active evenings in active datazones
      MeanActiveEvenings <-mean(result$n)
      
      #number of datazones with active evenings
      ActiveDataZones <- nrow(result)
      
      ##SD of activeevneings in datazones
      SD_ActiveEvenings <-sd(result$n)
      
      ##subset impressions in the home datazone to estimate home latitude and longitude
      eveningimpressions <- library[which(library$datazone==result[index,1]),]  

      #calculate the mean of the evening coordinates ion the datazone 
      longitude <- mean(eveningimpressions$impression_lng)
      latitude <- mean(eveningimpressions$impression_lat)
      
      ##add data to DF
      #insert ActiveDataZones
      Huq_homelocations_activeevenings[i,5] <- ActiveDataZones
      
      #insert MeanActiveEvenings (in datazones)
      Huq_homelocations_activeevenings[i,6] <- MeanActiveEvenings
      
      #insert SD_ActiveEvenings (in datazones)
      Huq_homelocations_activeevenings[i,7] <- SD_ActiveEvenings
      
      #insert latitude of home datazone
      Huq_homelocations_activeevenings[i,8] <- latitude
      
      #insert longitude of home datazone
      Huq_homelocations_activeevenings[i,9] <- longitude
      
      #insert number of evening impressions in home datazone
      Huq_homelocations_activeevenings[i,10] <- nrow(eveningimpressions)/nrow(library)
      
      #insert mean accuracy in home datazone
      Huq_homelocations_activeevenings[i,11] <- mean(eveningimpressions$impression_acc)
      
      #insert median hour of activity in home datazone
      Huq_homelocations_activeevenings[i,12] <- median(eveningimpressions$time)
      }
    
    ##IF the analysis returns MORE than 1 home datazone: add 
    if (length(index)>1){
      #add zone names to character vector
      zones <- as.character(result[index,1])
      #combine zones with / seperating them
      zones <- gsub(", ","/",toString(zones))
      #insert zones into DF
      Huq_homelocations_activeevenings[i,3] <- zones
      #insert mean number of points in returned datazones
      Huq_homelocations_activeevenings[i,4]  <- mean(result[index,2])
    }
    
    #save the dataframe and .csv file of analysis
    save(Huq_homelocations_activeevenings, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/1. Huq/2019/homelocations_huq_2019_raw.Rda")
    
    write.csv(Huq_homelocations_activeevenings, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/1. Huq/2019/homelocations_huq_2019_raw.csv")
}}
```

removing users with 1 active evening and save the data to database
```{r}
#load data
load(file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/1. Huq/2019/homelocations_huq_2019_raw.Rda")

#subset only those who have a home estimates
homelocations_huq_2019 <- Huq_homelocations_activeevenings %>% drop_na(Latitude)

#save to database as table
dbWriteTable(michael_DB, "homelocations_huq_2019",homelocations_huq_2019 ,overwrite = T )

#save rda and csv
save(homelocations_huq_2019 , file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/1. Huq/2019/homelocations_huq_2019.Rda")

write.csv(homelocations_huq_2019, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/1. Huq/2019/homelocations_huq_2019.csv")

#subset only those who have a home estimates and >1 active evening
homelocations_huq_2019_subset <- homelocations_huq_2019 %>% filter(`#EveInHome`>1)

#save to database as table
dbWriteTable(michael_DB, "homelocations_huq_2019_subset",homelocations_huq_2019_subset, overwrite = T )

#save rda and csv
save(homelocations_huq_2019_subset, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/1. Huq/2019/homelocations_huq_2019_subset.Rda")

write.csv(homelocations_huq_2019_subset, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/1. Huq/2019/homelocations_huq_2019_subset.csv")


```



2020

home estimates for all huq using pre-joined evening residential impressions 
```{r}
#get vector of users 
users<- dbGetQuery(michael_DB, "SELECT DISTINCT device_iid_hash FROM public.huq_2020_residential_datazones_evening")
users <- users[,1]

#set up dataframe to populate home location estimates on the sample of users
Huq_homelocations_activeevenings <- data.frame(matrix(ncol = 14, nrow = length(users)))

#name DF fields
names(Huq_homelocations_activeevenings) <- c("Device_iid_hash", "EveResImpressions", "Datazone", "#EveInHome", "ActiveRegions", "#MeanActiveEvenings", "SD_ActiveEvenings", "Latitude", "Longitude", "%EveResInHome", "MeanAccInHome", "MedianHourInHome", "daysindatabase", "TimeActive")
  
#loop through each user ID in sample, subsetting their library of impressions, enriching impressions and returning datazone with most active days. Also returning various metrics useful to determine the accuracy of the result

for (i in 1:length(users)){
    
  message(paste("Estimating home location for user: ", users[i]))
    
    # add user ID to DF
    Huq_homelocations_activeevenings[i,1] <- users[i]
    
    # subset user i's impressions from database via SQL query
    library <-dbGetQuery(michael_DB, 
                         paste("select * from public.huq_2020_residential_datazones_evening where device_iid_hash in ('", users[i],"')", sep = ""))
    
    #check the time in the database
    Huq_homelocations_activeevenings[i,13] <- as.numeric(max(library$timestamp)-min(library$timestamp))
    Huq_homelocations_activeevenings[i,14] <- Huq_homelocations_activeevenings[i,13]/365
    
    #subset ony accuracy under 70m which is around the median + 1 standard deviation from the mean
    library <- subset(library, library$impression_acc <100)
    
    # Convert date field to number in a new seperate colomn and adds time field 
    library$date <- strptime(library$timestamp, format="%Y-%m-%d")
    library$date_num <- as.numeric(library$date)
    library$time <- hour(library$timestamp)
    op <- options(digits.secs = 3)
    options(op)
  
    #count the total impressions from a user and add to DF
    Huq_homelocations_activeevenings[i,2] <- nrow(library)
    
        #if the user has no evening data then add to DF and move onto the next user
    if(nrow(library)<1){
      Huq_homelocations_activeevenings[i,3] <- "Poor accuracy"
      Huq_homelocations_activeevenings[i,4]  <- 0}
    
    else {

    # sorts library by ID and date
    library_ordered <- library[order(library$device_iid_hash, library$date_num, library$datazone), ] 
    
    #Adds columns of activeevenings and inpute 0 to rows to remove and 1 to activeevnings
    library_ordered$activeevenings <- ifelse(library_ordered$device_iid_hash == lag(library_ordered$device_iid_hash) & library_ordered$date_num == lag(library_ordered$date_num) &     library_ordered$datazone == lag(library_ordered$datazone), 0, 1)
    
    # creates activeevenings_index which is TRUE for activeevenings (rows) to keep. This is used as index to create a new DF with specific rows removed
    activeevenings_index <- library_ordered$activeevenings==1 | is.na(library_ordered$activeevenings)
    activeevenings <- library_ordered[activeevenings_index==TRUE, ]
    
    #count which datazone has the most active evenings and index
    result <- activeevenings %>% count(datazone)
    index <- which(result$n == max(result$n))

    # IF a single  datazone is returned: assign datazone code and count of active evenings to DF of users
    if (length(index)==1){
      #add datazone code to DF
      Huq_homelocations_activeevenings[i,3] <- result[index,1]
      #add count of active evenings in datazone
      Huq_homelocations_activeevenings[i,4] <- result[index,2]
      
      ##summary of results for all datazones which a users engaged in (Number of active datazones in a library, mean active data in all zones, SD...etc)
      ##mean activeevneings in datazones
      #subset datazones which the user is active in

      #calculate the mean active evenings in active datazones
      MeanActiveEvenings <-mean(result$n)
      
      #number of datazones with active evenings
      ActiveDataZones <- nrow(result)
      
      ##SD of activeevneings in datazones
      SD_ActiveEvenings <-sd(result$n)
      
      ##subset impressions in the home datazone to estimate home latitude and longitude
      eveningimpressions <- library[which(library$datazone==result[index,1]),]  

      #calculate the mean of the evening coordinates ion the datazone 
      longitude <- mean(eveningimpressions$impression_lng)
      latitude <- mean(eveningimpressions$impression_lat)
      
      ##add data to DF
      #insert ActiveDataZones
      Huq_homelocations_activeevenings[i,5] <- ActiveDataZones
      
      #insert MeanActiveEvenings (in datazones)
      Huq_homelocations_activeevenings[i,6] <- MeanActiveEvenings
      
      #insert SD_ActiveEvenings (in datazones)
      Huq_homelocations_activeevenings[i,7] <- SD_ActiveEvenings
      
      #insert latitude of home datazone
      Huq_homelocations_activeevenings[i,8] <- latitude
      
      #insert longitude of home datazone
      Huq_homelocations_activeevenings[i,9] <- longitude
      
      #insert number of evening impressions in home datazone
      Huq_homelocations_activeevenings[i,10] <- nrow(eveningimpressions)/nrow(library)
      
      #insert mean accuracy in home datazone
      Huq_homelocations_activeevenings[i,11] <- mean(eveningimpressions$impression_acc)
      
      #insert median hour of activity in home datazone
      Huq_homelocations_activeevenings[i,12] <- median(eveningimpressions$time)
      }
    
    ##IF the analysis returns MORE than 1 home datazone: add 
    if (length(index)>1){
      #add zone names to character vector
      zones <- as.character(result[index,1])
      #combine zones with / seperating them
      zones <- gsub(", ","/",toString(zones))
      #insert zones into DF
      Huq_homelocations_activeevenings[i,3] <- zones
      #insert mean number of points in returned datazones
      Huq_homelocations_activeevenings[i,4]  <- mean(result[index,2])
    }
    
    #save the dataframe and .csv file of analysis
    save(Huq_homelocations_activeevenings, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/1. Huq/2020/homelocations_huq_2020_raw.Rda")
    
    write.csv(Huq_homelocations_activeevenings, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/1. Huq/2020/homelocations_huq_2020_raw.csv")
}}
```

removing users with 1 active evening and save the data to database
```{r}
#load data
load(file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/1. Huq/2020/homelocations_huq_2020_raw.Rda")

#subset only those who have a home estimates
homelocations_huq_2020 <- Huq_homelocations_activeevenings %>% drop_na(Latitude)

#save to database as table
dbWriteTable(michael_DB, "homelocations_huq_2020",homelocations_huq_2020 ,overwrite = T )

#save rda and csv
save(homelocations_huq_2020 , file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/1. Huq/2020/homelocations_huq_2020.Rda")

write.csv(homelocations_huq_2020, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/1. Huq/2020/homelocations_huq_2020.csv")

#subset only those who have a home estimates and >1 active evening
homelocations_huq_2020_subset <- homelocations_huq_2020 %>% filter(`#EveInHome`>1)

#save to database as table
dbWriteTable(michael_DB, "homelocations_huq_2020_subset",homelocations_huq_2020_subset, overwrite = T )

#save rda and csv
save(homelocations_huq_2020_subset, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/1. Huq/2020/homelocations_huq_2020_subset.Rda")

write.csv(homelocations_huq_2020_subset, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/1. Huq/2020/homelocations_huq_2020_subset.csv")


```


2021

home estimates for all huq using pre-joined evening residential impressions 
```{r}
#get vector of users 
users<- dbGetQuery(michael_DB, "SELECT DISTINCT device_iid_hash FROM public.huq_2021_residential_datazones_evening")
users <- users[,1]

#set up dataframe to populate home location estimates on the sample of users
Huq_homelocations_activeevenings <- data.frame(matrix(ncol = 14, nrow = length(users)))

#name DF fields
names(Huq_homelocations_activeevenings) <- c("Device_iid_hash", "EveResImpressions", "Datazone", "#EveInHome", "ActiveRegions", "#MeanActiveEvenings", "SD_ActiveEvenings", "Latitude", "Longitude", "%EveResInHome", "MeanAccInHome", "MedianHourInHome", "daysindatabase", "TimeActive")
  
#loop through each user ID in sample, subsetting their library of impressions, enriching impressions and returning datazone with most active days. Also returning various metrics useful to determine the accuracy of the result

for (i in 1:length(users)){
    
  message(paste("Estimating home location for user: ", users[i]))
    
    # add user ID to DF
    Huq_homelocations_activeevenings[i,1] <- users[i]
    
    # subset user i's impressions from database via SQL query
    library <-dbGetQuery(michael_DB, 
                         paste("select * from public.huq_2021_residential_datazones_evening where device_iid_hash in ('", users[i],"')", sep = ""))
    
    #check the time in the database
    Huq_homelocations_activeevenings[i,13] <- as.numeric(max(library$timestamp)-min(library$timestamp))
    Huq_homelocations_activeevenings[i,14] <- Huq_homelocations_activeevenings[i,13]/365
    
    #subset ony accuracy under 70m which is around the median + 1 standard deviation from the mean
    library <- subset(library, library$impression_acc <100)
    
    # Convert date field to number in a new seperate colomn and adds time field 
    library$date <- strptime(library$timestamp, format="%Y-%m-%d")
    library$date_num <- as.numeric(library$date)
    library$time <- hour(library$timestamp)
    op <- options(digits.secs = 3)
    options(op)
  
    #count the total impressions from a user and add to DF
    Huq_homelocations_activeevenings[i,2] <- nrow(library)
    
        #if the user has no evening data then add to DF and move onto the next user
    if(nrow(library)<1){
      Huq_homelocations_activeevenings[i,3] <- "Poor accuracy"
      Huq_homelocations_activeevenings[i,4]  <- 0}
    
    else {

    # sorts library by ID and date
    library_ordered <- library[order(library$device_iid_hash, library$date_num, library$datazone), ] 
    
    #Adds columns of activeevenings and inpute 0 to rows to remove and 1 to activeevnings
    library_ordered$activeevenings <- ifelse(library_ordered$device_iid_hash == lag(library_ordered$device_iid_hash) & library_ordered$date_num == lag(library_ordered$date_num) &     library_ordered$datazone == lag(library_ordered$datazone), 0, 1)
    
    # creates activeevenings_index which is TRUE for activeevenings (rows) to keep. This is used as index to create a new DF with specific rows removed
    activeevenings_index <- library_ordered$activeevenings==1 | is.na(library_ordered$activeevenings)
    activeevenings <- library_ordered[activeevenings_index==TRUE, ]
    
    #count which datazone has the most active evenings and index
    result <- activeevenings %>% count(datazone)
    index <- which(result$n == max(result$n))

    # IF a single  datazone is returned: assign datazone code and count of active evenings to DF of users
    if (length(index)==1){
      #add datazone code to DF
      Huq_homelocations_activeevenings[i,3] <- result[index,1]
      #add count of active evenings in datazone
      Huq_homelocations_activeevenings[i,4] <- result[index,2]
      
      ##summary of results for all datazones which a users engaged in (Number of active datazones in a library, mean active data in all zones, SD...etc)
      ##mean activeevneings in datazones
      #subset datazones which the user is active in

      #calculate the mean active evenings in active datazones
      MeanActiveEvenings <-mean(result$n)
      
      #number of datazones with active evenings
      ActiveDataZones <- nrow(result)
      
      ##SD of activeevneings in datazones
      SD_ActiveEvenings <-sd(result$n)
      
      ##subset impressions in the home datazone to estimate home latitude and longitude
      eveningimpressions <- library[which(library$datazone==result[index,1]),]  

      #calculate the mean of the evening coordinates ion the datazone 
      longitude <- mean(eveningimpressions$impression_lng)
      latitude <- mean(eveningimpressions$impression_lat)
      
      ##add data to DF
      #insert ActiveDataZones
      Huq_homelocations_activeevenings[i,5] <- ActiveDataZones
      
      #insert MeanActiveEvenings (in datazones)
      Huq_homelocations_activeevenings[i,6] <- MeanActiveEvenings
      
      #insert SD_ActiveEvenings (in datazones)
      Huq_homelocations_activeevenings[i,7] <- SD_ActiveEvenings
      
      #insert latitude of home datazone
      Huq_homelocations_activeevenings[i,8] <- latitude
      
      #insert longitude of home datazone
      Huq_homelocations_activeevenings[i,9] <- longitude
      
      #insert number of evening impressions in home datazone
      Huq_homelocations_activeevenings[i,10] <- nrow(eveningimpressions)/nrow(library)
      
      #insert mean accuracy in home datazone
      Huq_homelocations_activeevenings[i,11] <- mean(eveningimpressions$impression_acc)
      
      #insert median hour of activity in home datazone
      Huq_homelocations_activeevenings[i,12] <- median(eveningimpressions$time)
      }
    
    ##IF the analysis returns MORE than 1 home datazone: add 
    if (length(index)>1){
      #add zone names to character vector
      zones <- as.character(result[index,1])
      #combine zones with / seperating them
      zones <- gsub(", ","/",toString(zones))
      #insert zones into DF
      Huq_homelocations_activeevenings[i,3] <- zones
      #insert mean number of points in returned datazones
      Huq_homelocations_activeevenings[i,4]  <- mean(result[index,2])
    }
    
    #save the dataframe and .csv file of analysis
    save(Huq_homelocations_activeevenings, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/1. Huq/2021/homelocations_huq_2021_raw.Rda")
    
    write.csv(Huq_homelocations_activeevenings, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/1. Huq/2021/homelocations_huq_2021_raw.csv")
}}
```

removing users with 1 active evening and save the data to database
```{r}
#load data
load(file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/1. Huq/2021/homelocations_huq_2021_raw.Rda")

#subset only those who have a home estimates
homelocations_huq_2021 <- Huq_homelocations_activeevenings %>% drop_na(Latitude)

#save to database as table
dbWriteTable(michael_DB, "homelocations_huq_2021",homelocations_huq_2021 ,overwrite = T )

#save rda and csv
save(homelocations_huq_2021 , file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/1. Huq/2021/homelocations_huq_2021.Rda")

write.csv(homelocations_huq_2021, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/1. Huq/2021/homelocations_huq_2021.csv")

#subset only those who have a home estimates and >1 active evening
homelocations_huq_2021_subset <- homelocations_huq_2021 %>% filter(`#EveInHome`>1)

#save to database as table
dbWriteTable(michael_DB, "homelocations_huq_2021_subset",homelocations_huq_2021_subset, overwrite = T )

#save rda and csv
save(homelocations_huq_2021_subset, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/1. Huq/2021/homelocations_huq_2021_subset.Rda")

write.csv(homelocations_huq_2021_subset, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/1. Huq/2021/homelocations_huq_2021_subset.csv")


```


TAMOCO

2019-2021 (loops through each year and each month)
home estimates for tamoco (all years)  using pre-joined evening residential impressions 
```{r}
#set year and month vectors
year <- c("2019", "2020", "2021")
month <- c("jan", "feb", "mar", "apr", "may", "Jun", "jul", "aug", "sep", "oct", "nov", "dec")

#loop through each year
for (y in 1:length(year)){
  message(paste("Beginning year: ",year[y]))

  #loop through each month
  for (m in c(1:12)){

    message(paste("Beginning month: ",month[m], "of ", year[y]))
    
    #get vector of users for months
    users<- dbGetQuery(michael_DB, paste("SELECT DISTINCT hashed_user_id FROM tamoco_", year[y],"_",month[m], sep = ""))
    users <- users[,1] 
    
    #set up dataframe to populate home location estimates on the sample of users
    Tamoco_homelocations_activeevenings <- data.frame(matrix(ncol = 14, nrow = length(users)))
    
    #name DF fields
    names(Tamoco_homelocations_activeevenings) <- c("Device_iid_hash", "EveResImpressions", "Datazone", "#EveInHome", "ActiveRegions", "#MeanActiveEvenings", "SD_ActiveEvenings", "Latitude", "Longitude", "%EveInHome", "MeanAccInHome", "MedianHourInHome", "daysindatabase", "TimeActive")
      
    #loop through each user ID in sample, subsetting their library of impressions, enriching impressions and returning datazone with most active days. Also returning various metrics useful to determine the accuracy of the result
    for (i in 1:length(users)){
        
      message(paste("Estimating home location for user: ", users[i]))
        
        # add user ID to DF
        Tamoco_homelocations_activeevenings[i,1] <- users[i]
        
        # subset user i's impressions from database via SQL query
        library <- dbGetQuery(michael_DB, paste("select * from tamoco_",year[y],"_",month[m]," where hashed_user_id in ('", users[i],"')", sep = ""))
        
        #check the time in the database
        Tamoco_homelocations_activeevenings[i,13] <- as.numeric(max(library$sdk_ts)-min(library$sdk_ts))
        Tamoco_homelocations_activeevenings[i,14] <- Tamoco_homelocations_activeevenings[i,13]/31
     
        #subset only accuracy under 70m which is around the median + 1 standard deviation from the mean
        library <- subset(library, library$accuracy <100)
        
        # Convert date field to number in a new seperate colomn and adds time field 
        library$date <- strptime(library$sdk_ts, format="%Y-%m-%d")
        library$date_num <- as.numeric(library$date)
        library$time <- hour(library$sdk_ts)
        op <- options(digits.secs = 3)
        options(op)
      
        #count the total impressions from a user and add to DF
        Tamoco_homelocations_activeevenings[i,2] <- nrow(library)
        
            #if the user has no evening data then add to DF and move onto the next user
        if(nrow(library)<1){
          Tamoco_homelocations_activeevenings[i,3] <- "Poor accuracy"
          Tamoco_homelocations_activeevenings[i,4]  <- 0}
        
        else {
    
        # sorts library by ID, date and datazone
        library_ordered <- library[order(library$hashed_user_id, library$date_num, library$datazone), ] 
        
        #Adds columns of activeevenings and inpute 0 to rows to remove and 1 to activeevnings
        library_ordered$activeevenings <- ifelse(library_ordered$hashed_user_id == lag(library_ordered$hashed_user_id) & library_ordered$date_num == lag(library_ordered$date_num) &     library_ordered$datazone == lag(library_ordered$datazone), 0, 1)
        
        # creates activeevenings_index which is TRUE for activeevenings (rows) to keep. This is used as index to create a new DF with specific rows removed
        activeevenings_index <- library_ordered$activeevenings==1 | is.na(library_ordered$activeevenings)
        activeevenings <- library_ordered[activeevenings_index==TRUE, ]
        
        #count which datazone has the most active evenings and index
        result <- activeevenings %>% count(datazone)
        index <- which(result$n == max(result$n))
    
        # IF a single  datazone is returned: assign datazone code and count of active evenings to DF of users
        if (length(index)==1){
          #add datazone code to DF
          Tamoco_homelocations_activeevenings[i,3] <- result[index,1]
          #add count of active evenings in datazone
          Tamoco_homelocations_activeevenings[i,4] <- result[index,2]
          
          ##summary of results for all datazones which a users engaged in (Number of active datazones in a library, mean active data in all zones, SD...etc)
    
          #calculate the mean active evenings in active datazones
          MeanActiveEvenings <-mean(result$n)
          
          #number of datazones with active evenings
          ActiveDataZones <- nrow(result)
          
          ##SD of activeevneings in datazones
          SD_ActiveEvenings <-sd(result$n)
          
          ##subset impressions in the home datazone to estimate home latitude and longitude
          eveningimpressions <- library[which(library$datazone==result[index,1]),]  
    
          #calculate the mean of the evening coordinates ion the datazone 
          longitude <- mean(eveningimpressions$longitude)
          latitude <- mean(eveningimpressions$latitude)
          
          ##add data to DF
          #insert ActiveDataZones
          Tamoco_homelocations_activeevenings[i,5] <- ActiveDataZones
          
          #insert MeanActiveEvenings (in datazones)
          Tamoco_homelocations_activeevenings[i,6] <- MeanActiveEvenings
          
          #insert SD_ActiveEvenings (in datazones)
          Tamoco_homelocations_activeevenings[i,7] <- SD_ActiveEvenings
          
          #insert latitude of home datazone
          Tamoco_homelocations_activeevenings[i,8] <- latitude
          
          #insert longitude of home datazone
          Tamoco_homelocations_activeevenings[i,9] <- longitude
          
          #insert number of evening impressions in home datazone
          Tamoco_homelocations_activeevenings[i,10] <- nrow(eveningimpressions)/nrow(library)
          
          #insert mean accuracy in home datazone
          Tamoco_homelocations_activeevenings[i,11] <- mean(eveningimpressions$accuracy)
          
          #insert median hour of activity in home datazone
          Tamoco_homelocations_activeevenings[i,12] <- median(eveningimpressions$time)
          }
        
        ##IF the analysis returns MORE than 1 home datazone: add 
        if (length(index)>1){
          #add zone names to character vector
          zones <- as.character(result[index,1])
          #combine zones with / seperating them
          zones <- gsub(", ","/",toString(zones))
          #insert zones into DF
          Tamoco_homelocations_activeevenings[i,3] <- zones
          #insert mean number of points in returned datazones
          Tamoco_homelocations_activeevenings[i,4]  <- mean(result[index,2])
        }
        
        #save the dataframe and .csv file of analysis
        save(Tamoco_homelocations_activeevenings, file=paste("C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/", year[y],"/Monthly data/homelocations_tamoco_",year[y],"_",month[m],".Rda", sep=""))
        
        write.csv(Tamoco_homelocations_activeevenings, file=paste("C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/", year[y],"/Monthly data/homelocations_tamoco_",year[y],"_",month[m],".csv", sep=""))
        }}}}

beep(sound=4)
```


2019

load tamoco for each year, join month, save, subset to only those with home locations and save to database
```{r}
#set directory by year where the Rda files are stored by month
setwd("C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/2019")

#Creates a list of file names with extentions
file_list <- list.files(path = "C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/2019/Monthly data", pattern = "*.Rda", full.names = TRUE)

#Creates empty DF to fill
homelocations_tamoco_2019_raw <- Tamoco_homelocations_activeevenings[FALSE,]

#loop files and bind
for (i in 1:length(file_list)){
    message(paste("joining: ", file_list[i]))
    load(file_list[i])
    names(Tamoco_homelocations_activeevenings)[10] <- "%EveResInHome"
    homelocations_tamoco_2019_raw <- rbind(homelocations_tamoco_2019_raw, Tamoco_homelocations_activeevenings)
 }
    
#save raw joined data as rda and csv  
save(homelocations_tamoco_2019_raw, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/2019/homelocations_tamoco_2019_raw.Rda")

write.csv(homelocations_tamoco_2019_raw, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/2019/homelocations_tamoco_2019_raw.csv")

#subset only those who have a home estimates
homelocations_tamoco_2019 <- homelocations_tamoco_2019_raw %>% tidyr::drop_na(Latitude)

#save to database as table
dbWriteTable(michael_DB, "homelocations_tamoco_2019",homelocations_tamoco_2019,overwrite = T )

#save rda and csv
save(homelocations_tamoco_2019, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/2019/homelocations_tamoco_2019.Rda")

write.csv(homelocations_tamoco_2019, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/2019/homelocations_tamoco_2019.csv")

#subset only those who have a home estimates and >1 active evening
homelocations_tamoco_2019_subset <- homelocations_tamoco_2019 %>% filter(`#EveInHome`>1)

#save to database as table
dbWriteTable(michael_DB, "homelocations_tamoco_2019_subset",homelocations_tamoco_2019_subset, overwrite = T )

#save rda and csv
save(homelocations_tamoco_2019_subset, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/2019/homelocations_tamoco_2019_subset.Rda")

write.csv(homelocations_tamoco_2019_subset, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/2019/homelocations_tamoco_2019_subset.csv")


```

load tamoco for each month, subset to only those with home locations and save into database
```{r}
#set directory by year where the Rda files are stored by month
setwd("C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/2019/monthly data")

#Creates a list of file names with extentions
file_list <- list.files(path = "C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/2019/Monthly data", pattern = "*.Rda", full.names = TRUE)

#month names
months <- c("Apr", "Aug", "Dec", "Feb", "Jan", "Jul", "Jun", "Mar", "May", "Nov", "Oct", "Sep")

#loop files and bind
for (i in 1:length(file_list)){
    message(paste("adding: ", file_list[i]))
    load(file_list[i])
    homelocations_tamoco_2019_monthly <- Tamoco_homelocations_activeevenings %>% tidyr::drop_na(Latitude)
    homelocations_tamoco_2019_monthly_subset <- homelocations_tamoco_2019_monthly %>% filter(`#EveInHome`>1)
    dbWriteTable(michael_DB, paste("homelocations_tamoco_2019_", months[i], sep = ""), homelocations_tamoco_2019_monthly_subset, overwrite = T )
 }

```



2020

load tamoco for each year, join month, save, subset to only those with home locations and save to database
```{r}
#set directory by year where the Rda files are stored by month
setwd("C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/2020")

#Creates a list of file names with extentions
file_list <- list.files(path = "C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/2020/Monthly data", pattern = "*.Rda", full.names = TRUE)

#Creates empty DF to fill
homelocations_tamoco_2020_raw <- Tamoco_homelocations_activeevenings[FALSE,]

#loop files and bind
for (i in 1:length(file_list)){
    message(paste("joining: ", file_list[i]))
    load(file_list[i])
    names(Tamoco_homelocations_activeevenings)[10] <- "%EveResInHome"
    homelocations_tamoco_2020_raw <- rbind(homelocations_tamoco_2020_raw, Tamoco_homelocations_activeevenings)
 }
    
#save raw joined data as rda and csv  
save(homelocations_tamoco_2020_raw, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/2020/homelocations_tamoco_2020_raw.Rda")
write.csv(homelocations_tamoco_2020_raw, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/2020/homelocations_tamoco_2020_raw.csv")

#subset only those who have a home estimates
homelocations_tamoco_2020 <- homelocations_tamoco_2020_raw %>% tidyr::drop_na(Latitude)

#save to database as table
dbWriteTable(michael_DB, "homelocations_tamoco_2020",homelocations_tamoco_2020,overwrite = T )

#save rda and csv
save(homelocations_tamoco_2020, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/2020/homelocations_tamoco_2020.Rda")
write.csv(homelocations_tamoco_2020, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/2020/homelocations_tamoco_2020.csv")

#subset only those who have a home estimates and >1 active evening
homelocations_tamoco_2020_subset <- homelocations_tamoco_2020 %>% filter(`#EveInHome`>1)

#save to database as table
dbWriteTable(michael_DB, "homelocations_tamoco_2020_subset",homelocations_tamoco_2020_subset, overwrite = T )

#save rda and csv
save(homelocations_tamoco_2020_subset, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/2020/homelocations_tamoco_2020_subset.Rda")
write.csv(homelocations_tamoco_2020_subset, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/2020/homelocations_tamoco_2020_subset.csv")


```

load tamoco for each month, subset to only those with home locations and save into database
```{r}
#set directory by year where the Rda files are stored by month
setwd("C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/2020/monthly data")

#Creates a list of file names with extentions
file_list <- list.files(path = "C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/2020/Monthly data", pattern = "*.Rda", full.names = TRUE)

#month names
months <- c("Apr", "Aug", "Dec", "Feb", "Jan", "Jul", "Jun", "Mar", "May", "Nov", "Oct", "Sep")

#loop files and bind
for (i in 1:length(file_list)){
    message(paste("adding: ", file_list[i]))
    load(file_list[i])
    homelocations_tamoco_2020_monthly <- Tamoco_homelocations_activeevenings %>% tidyr::drop_na(Latitude)
    homelocations_tamoco_2020_monthly_subset <- homelocations_tamoco_2020_monthly %>% filter(`#EveInHome`>1)
    dbWriteTable(michael_DB, paste("homelocations_tamoco_2020_", months[i], sep = ""), homelocations_tamoco_2020_monthly_subset, overwrite = T )
 }

```


2021

load tamoco for each year, join month, save, subset to only those with home locations and save to database
```{r}
#set directory by year where the Rda files are stored by month
setwd("C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/2021")

#Creates a list of file names with extentions
file_list <- list.files(path = "C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/2021/Monthly data", pattern = "*.Rda", full.names = TRUE)

#Creates empty DF to fill
homelocations_tamoco_2021_raw <- Tamoco_homelocations_activeevenings[FALSE,]

#loop files and bind
for (i in 1:length(file_list)){
    message(paste("joining: ", file_list[i]))
    load(file_list[i])
    names(Tamoco_homelocations_activeevenings)[10] <- "%EveResInHome"
    homelocations_tamoco_2021_raw <- rbind(homelocations_tamoco_2021_raw, Tamoco_homelocations_activeevenings)
 }
    
#save raw joined data as rda and csv  
save(homelocations_tamoco_2021_raw, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/2021/homelocations_tamoco_2021_raw.Rda")
write.csv(homelocations_tamoco_2021_raw, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/2021/homelocations_tamoco_2021_raw.csv")

#subset only those who have a home estimates
homelocations_tamoco_2021 <- homelocations_tamoco_2021_raw %>% tidyr::drop_na(Latitude)

#save to database as table
dbWriteTable(michael_DB, "homelocations_tamoco_2021",homelocations_tamoco_2021,overwrite = T )

#save rda and csv
save(homelocations_tamoco_2021, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/2021/homelocations_tamoco_2021.Rda")
write.csv(homelocations_tamoco_2021, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/2021/homelocations_tamoco_2021.csv")

#subset only those who have a home estimates and >1 active evening
homelocations_tamoco_2021_subset <- homelocations_tamoco_2021 %>% filter(`#EveInHome`>1)

#save to database as table
dbWriteTable(michael_DB, "homelocations_tamoco_2021_subset",homelocations_tamoco_2021_subset, overwrite = T )

#save rda and csv
save(homelocations_tamoco_2021_subset, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/2021/homelocations_tamoco_2021_subset.Rda")
write.csv(homelocations_tamoco_2021_subset, file="C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/2021/homelocations_tamoco_2021_subset.csv")


```

load tamoco for each month, subset to only those with home locations and save into database
```{r}
#set directory by year where the Rda files are stored by month
setwd("C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/2021/monthly data")

#Creates a list of file names with extentions
file_list <- list.files(path = "C:/Users/ms619t/OneDrive - University of Glasgow/Urban Big Data Centre/4. Home location analysis/2. Tamoco/Alg_landuse/2021/Monthly data", pattern = "*.Rda", full.names = TRUE)

#month names
months <- c("Apr", "Aug", "Dec", "Feb", "Jan", "Jul", "Jun", "Mar", "May", "Nov", "Oct", "Sep")

#loop files and bind
for (i in 1:length(file_list)){
    message(paste("adding: ", file_list[i]))
    load(file_list[i])
    homelocations_tamoco_2021_monthly <- Tamoco_homelocations_activeevenings %>% tidyr::drop_na(Latitude)
    homelocations_tamoco_2021_monthly_subset <- homelocations_tamoco_2021_monthly %>% filter(`#EveInHome`>1)
    dbWriteTable(michael_DB, paste("homelocations_tamoco_2021_", months[i], sep = ""), homelocations_tamoco_2021_monthly_subset, overwrite = T )
 }

```

The post-processing steps 2.5 to 2.9 in the code 'Complete SQL code.sql' should be run after this script.